{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assignment 4 - Classifying handwritten digits using Numpy and Tensorflow.\n",
    "--\n",
    "For this assignment, you should complete the exercises in this notebook. It is similar to the notebook posted for binary logistic regression. \n",
    "\n",
    "Look for requests containing the text **\"your code\"**. E.g. \"put your code here\", \"replace None by your code\", etc.\n",
    "If there is no such request in a cell, just run the cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "from scipy import ndimage\n",
    "import time\n",
    "import pickle\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let's load the dataset of handwritten digits from a Python pickle file. \n",
    "# For information on pickle files, see: https://pythontips.com/2013/08/02/what-is-pickle-in-python\n",
    "# The pickle file contains 55,000 training images and their labels as well as\n",
    "# 10,000 test images and their labels.\n",
    "\n",
    "fileObject = open(\"mnist_nonbin_classification.pickle\",'rb')  \n",
    "X,Y,X_test,Y_test = pickle.load(fileObject)\n",
    "fileObject.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "7\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nZAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1fa0eecd4a8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Let's view some images\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(X[1,:].reshape((28,28)), cmap='Greys')\n",
    "\n",
    "print(Y[1])\n",
    "print(Y_test[0])\n",
    "\n",
    "# Feel free to display other images by changing the index 0 above to some other index. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset shape (55000, 784) (55000, 1)\n",
      "Test dataset shape (10000, 784) (10000, 1)\n",
      "Y = [[7]\n",
      " [3]\n",
      " [4]\n",
      " ..., \n",
      " [5]\n",
      " [6]\n",
      " [8]]\n"
     ]
    }
   ],
   "source": [
    "# We will reshape the Y arrays so that they are not rank 1 arrays but rank 2 arrays. \n",
    "# They should be rank 2 arrays.\n",
    "\n",
    "Y = Y.reshape((Y.shape[0],1))\n",
    "Y_test = Y_test.reshape((Y_test.shape[0],1))\n",
    "\n",
    "print(\"Train dataset shape\", X.shape, Y.shape)\n",
    "print(\"Test dataset shape\", X_test.shape, Y_test.shape)\n",
    "\n",
    "print(\"Y =\", Y)\n",
    "\n",
    "m   = X.shape[0] \n",
    "n_x = X.shape[1]\n",
    "\n",
    "C = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise 1 - One Hot Encoding\n",
    "--\n",
    "\n",
    "Convert Y to \"one-hot\" encoding. E.g. if the original Y is \n",
    "$$\n",
    "Y = \\begin{bmatrix}\n",
    "    1 \\\\\n",
    "    5 \\\\\n",
    "    9  \n",
    "\\end{bmatrix}\n",
    "$$\n",
    "the new Y should be\n",
    "$$\n",
    "Y = \\begin{bmatrix}\n",
    "    0 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0\\\\\n",
    "    0 & 0 & 0 & 0 & 0 & 1 & 0 & 0 & 0 & 0\\\\\n",
    "    0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 1\\\\ \n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Toward this goal, let's check what np.arange(C) produces\n",
    "np.arange(C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[7],\n",
       "       [3],\n",
       "       [4],\n",
       "       ..., \n",
       "       [5],\n",
       "       [6],\n",
       "       [8]], dtype=uint8)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's see again what Y is\n",
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "broadcasted np.arange(C) = \n",
      " [[0 1 2 ..., 7 8 9]\n",
      " [0 1 2 ..., 7 8 9]\n",
      " [0 1 2 ..., 7 8 9]\n",
      " ..., \n",
      " [0 1 2 ..., 7 8 9]\n",
      " [0 1 2 ..., 7 8 9]\n",
      " [0 1 2 ..., 7 8 9]]\n",
      "broadcasted Y = \n",
      " [[7 7 7 ..., 7 7 7]\n",
      " [3 3 3 ..., 3 3 3]\n",
      " [4 4 4 ..., 4 4 4]\n",
      " ..., \n",
      " [5 5 5 ..., 5 5 5]\n",
      " [6 6 6 ..., 6 6 6]\n",
      " [8 8 8 ..., 8 8 8]]\n"
     ]
    }
   ],
   "source": [
    "# What would broadcasting these arrays together would look like? \n",
    "# Let's check\n",
    "\n",
    "a,b = np.broadcast_arrays(np.arange(C), Y)\n",
    "\n",
    "print(\"broadcasted np.arange(C) = \\n\", a)\n",
    "print(\"broadcasted Y = \\n\", b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Y_new= [[False False False ...,  True False False]\n",
      " [False False False ..., False False False]\n",
      " [False False False ..., False False False]\n",
      " ..., \n",
      " [False False False ..., False False False]\n",
      " [False False False ..., False False False]\n",
      " [False False False ..., False  True False]]\n",
      "Y= [[0 0 0 ..., 1 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " ..., \n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 1 0]]\n",
      "Y_new_test= [[False False False ...,  True False False]\n",
      " [False False  True ..., False False False]\n",
      " [False  True False ..., False False False]\n",
      " ..., \n",
      " [False False False ..., False False False]\n",
      " [False False False ..., False False False]\n",
      " [False False False ..., False False False]]\n",
      "Y_test= [[0 0 0 ..., 1 0 0]\n",
      " [0 0 1 ..., 0 0 0]\n",
      " [0 1 0 ..., 0 0 0]\n",
      " ..., \n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "# If we compare np.arange(C) with Y using the equality sign ==, \n",
    "# numpy broadcasting will do its magic to give us what we want. \n",
    "# Try it out. Then assign the result to a new variable Y_new. \n",
    "# Don't worry for the \"True\" and \"False\" values looking like strings. \n",
    "# They behave in fact like numbers, i.e. True is 1, False is 0.\n",
    "# Finally, assign Y_new to Y so that we have to deal with Y in rest of the notebook.\n",
    "# Cast the boolean values of Y_new to integer by calling Y_new.astype(int)\n",
    "\n",
    "Y_new =   np.arange(C) == Y\n",
    "print(\"Y_new=\", Y_new)\n",
    "Y = Y_new.astype(int)\n",
    "print(\"Y=\",Y)\n",
    "\n",
    "Y_new_test =   np.arange(C) == Y_test\n",
    "print(\"Y_new_test=\", Y_new_test)\n",
    "Y_test = Y_new_test.astype(int)\n",
    "print(\"Y_test=\",Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise 2 - The Softmax Function\n",
    "--\n",
    "(Adapted from Andrew Ng's exercise in Coursera, deeplearning.ai)\n",
    "\n",
    "Implement a softmax function using numpy. Softmax is a normalizing function used when the algorithm needs to classify two or more classes. \n",
    "\n",
    "**Instructions**:\n",
    "- for $x \\in \\mathbb{R}^{1\\times n}$ \n",
    "$$\\mbox{softmax}(x) = \\mbox{softmax}\\left(\\begin{bmatrix}\n",
    "    x_1  &&\n",
    "    x_2 &&\n",
    "    ...  &&\n",
    "    x_n  \n",
    "\\end{bmatrix}\\right) = \\begin{bmatrix}\n",
    "     \\frac{e^{x_1}}{\\sum_{j}e^{x_j}}  &&\n",
    "    \\frac{e^{x_2}}{\\sum_{j}e^{x_j}}  &&\n",
    "    ...  &&\n",
    "    \\frac{e^{x_n}}{\\sum_{j}e^{x_j}} \n",
    "\\end{bmatrix} $$ \n",
    "\n",
    "- for a matrix $x \\in \\mathbb{R}^{m \\times n}$  \n",
    "$$\\mbox{softmax}(x) = \\mbox{softmax}\\begin{bmatrix}\n",
    "    x_{11} & x_{12} & x_{13} & \\dots  & x_{1n} \\\\\n",
    "    x_{21} & x_{22} & x_{23} & \\dots  & x_{2n} \\\\\n",
    "    \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "    x_{m1} & x_{m2} & x_{m3} & \\dots  & x_{mn}\n",
    "\\end{bmatrix} = \\begin{bmatrix}\n",
    "    \\frac{e^{x_{11}}}{\\sum_{j}e^{x_{1j}}} & \\frac{e^{x_{12}}}{\\sum_{j}e^{x_{1j}}} & \\frac{e^{x_{13}}}{\\sum_{j}e^{x_{1j}}} & \\dots  & \\frac{e^{x_{1n}}}{\\sum_{j}e^{x_{1j}}} \\\\\n",
    "    \\frac{e^{x_{21}}}{\\sum_{j}e^{x_{2j}}} & \\frac{e^{x_{22}}}{\\sum_{j}e^{x_{2j}}} & \\frac{e^{x_{23}}}{\\sum_{j}e^{x_{2j}}} & \\dots  & \\frac{e^{x_{2n}}}{\\sum_{j}e^{x_{2j}}} \\\\\n",
    "    \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "    \\frac{e^{x_{m1}}}{\\sum_{j}e^{x_{mj}}} & \\frac{e^{x_{m2}}}{\\sum_{j}e^{x_{mj}}} & \\frac{e^{x_{m3}}}{\\sum_{j}e^{x_{mj}}} & \\dots  & \\frac{e^{x_{mn}}}{\\sum_{j}e^{x_{mj}}}\n",
    "\\end{bmatrix} = \\begin{pmatrix}\n",
    "    \\mbox{softmax}\\text{(first row of x)}  \\\\\n",
    "    \\mbox{softmax}\\text{(second row of x)} \\\\\n",
    "    ...  \\\\\n",
    "    \\mbox{softmax}\\text{(last row of x)} \\\\\n",
    "\\end{pmatrix} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "softmax(x) = [[  6.74508059e-02   1.83350300e-01   4.98397788e-01   6.74508059e-02\n",
      "    1.83350300e-01]\n",
      " [  9.81452586e-01   1.79759312e-02   3.29240664e-04   1.21120871e-04\n",
      "    1.21120871e-04]]\n"
     ]
    }
   ],
   "source": [
    "def softmax(x):\n",
    "    # Create an array x_exp by applying np.exp() element-wise to x. \n",
    "    x_exp = np.exp(x)\n",
    "\n",
    "    # Create an array x_sum that contains the sum of each row of x_exp. \n",
    "    # Use np.sum(..., axis = 1, keepdims = True).\n",
    "    x_sum = np.sum(x_exp,axis=1,keepdims=True)\n",
    "    \n",
    "    # Compute softmax(x) by dividing x_exp by x_sum. It should automatically use numpy broadcasting.\n",
    "    # Return this array.\n",
    "    return x_exp / x_sum\n",
    "\n",
    "\n",
    "# Let's test\n",
    "x = np.array([\n",
    "    [1, 2, 3, 1, 2],\n",
    "    [9, 5, 1, 0 ,0]])\n",
    "print(\"softmax(x) = \" + str(softmax(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise 3 - Compute one semi-vectorized iteration for softmax\n",
    "--\n",
    "Perform one semi-vectorized iteration of gradient descent for the softmax classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y [[0 0 0 0 0 0 0 1 0 0]]\n",
      "z [[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]]\n",
      "a [[ 0.1  0.1  0.1  0.1  0.1  0.1  0.1  0.1  0.1  0.1]]\n",
      "J 2.30258509299\n",
      "dz [[ 0.1  0.1  0.1  0.1  0.1  0.1  0.1 -0.9  0.1  0.1]]\n",
      "dw [[ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]]\n",
      "db [[ 0.1  0.1  0.1  0.1  0.1  0.1  0.1 -0.9  0.1  0.1]]\n"
     ]
    }
   ],
   "source": [
    "# First do this for only one training example (the first one, i=0).\n",
    "# Print out everything you compute, e.g. print(\"z\", z), print(\"a\", a), etc. \n",
    "\n",
    "J = 0\n",
    "w = np.zeros((n_x,C))\n",
    "b = np.zeros((1,C))\n",
    "\n",
    "dw = np.zeros((n_x,C));\n",
    "db = np.zeros((1,C));\n",
    "\n",
    "i = 0;\n",
    "\n",
    "x = X[i:i+1, : ]  #x is [1,784]\n",
    "y = Y[i:i+1, : ]\n",
    "print(\"y\", y)\n",
    "\n",
    "z = x @ w + b\n",
    "print(\"z\", z)\n",
    "a = softmax(z)\n",
    "print(\"a\", a)\n",
    "J += np.sum(-y * np.log(a))\n",
    "print(\"J\", J)\n",
    "\n",
    "dz = a - y\n",
    "print(\"dz\", dz)\n",
    "dw += x.T * dz\n",
    "print(\"dw\", dw)\n",
    "db += dz\n",
    "print(\"db\", db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "J =  2.302585093\n",
      "Time needed:  4.0044028759002686\n"
     ]
    }
   ],
   "source": [
    "#one iteration, semi-vectorized\n",
    "\n",
    "J = 0\n",
    "w = np.zeros((n_x,C))\n",
    "b = np.zeros((1,C))\n",
    "\n",
    "dw = np.zeros((n_x,C));\n",
    "db = np.zeros((1,C));\n",
    "\n",
    "alpha = 0.1\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for i in range(m):\n",
    "    x = X[i:i+1, : ]  #x is [1,784]\n",
    "    y = Y[i:i+1, : ]\n",
    "\n",
    "    z = x @ w + b\n",
    "    a = softmax(z)\n",
    "    J += np.sum(-y*np.log(a))\n",
    "    \n",
    "    dz = a - y\n",
    "    dw += x.T * dz\n",
    "    db += dz\n",
    "\n",
    "J /= m; dw /= m; db /= m\n",
    "\n",
    "w -= alpha*dw\n",
    "b -= alpha*db\n",
    "\n",
    "print(\"J = \", J)\n",
    "print(\"Time needed: \", time.time() - start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise 4 - Compute one fully-vectorized iteration for softmax\n",
    "--\n",
    "Perform one fully-vectorized iteration of gradient descent for the softmax classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "J =  2.30258509299\n",
      "Time needed:  0.6198852062225342\n"
     ]
    }
   ],
   "source": [
    "#one iteration, fully vectorized, no for loop\n",
    "\n",
    "J = 0\n",
    "w = np.zeros((n_x,C))\n",
    "b = np.zeros((1,C))\n",
    "\n",
    "alpha = 0.1\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Convention: \n",
    "# Use capital letter when the variable is for the whole dataset of m train examples.\n",
    "\n",
    "# X is (55000,784), Y is (55000,10), w is (784,10), b is (1,10)  \n",
    "Z = X @ w + b               # Z is  (55000, 10)\n",
    "A = softmax(Z)              # A is  (55000, 10)\n",
    "J = -(1/m) * np.sum(Y * np.log(A)) \n",
    "\n",
    "dZ = A - Y                 # dZ is (55000, 10)\n",
    "\n",
    "\n",
    "dw = (1/m) * X.T @ dZ                #dw is (784, 10) \n",
    "db = (1/m) * np.sum(dZ,axis=0, keepdims=True)\n",
    "\n",
    "w -= alpha*dw\n",
    "b -= alpha*db\n",
    "\n",
    "print(\"J = \", J)\n",
    "print(\"Time needed: \", time.time() - start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe that the time of the fully vectorized version is almost one order of magnitude smaller. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise 5 - Compute several fully-vectorized iterations for softmax\n",
    "--\n",
    "Perform 100 fully-vectorized iterations of gradient descent for the softmax classification.\n",
    "Start with doing 10 iterations first, check the accuracy you achieve, then try for 100 iterations. \n",
    "Print out the cost after each iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 2.30258509299\n",
      "1 2.19706377599\n",
      "2 2.10085951391\n",
      "3 2.0120441413\n",
      "4 1.92974131203\n",
      "5 1.85342090129\n",
      "6 1.78266241239\n",
      "7 1.71707943589\n",
      "8 1.65629942394\n",
      "9 1.59996131888\n"
     ]
    }
   ],
   "source": [
    "J = 0\n",
    "w = np.zeros((n_x,C))\n",
    "b = np.zeros((1,C))\n",
    "\n",
    "alpha = 0.1\n",
    "\n",
    "# Convention: \n",
    "# Use capital letter when the variable is for the whole dataset of m train examples.\n",
    "\n",
    "for iter in range(10):\n",
    "    # X is (55000,784), Y is (55000,10), w is (784,10), b is (1,10)  \n",
    "    Z = X @ w + b               # Z is  (55000, 10)\n",
    "    A = softmax(Z)              # A is  (55000, 10)\n",
    "    J = -(1/m) * np.sum(Y * np.log(A)) \n",
    "    print(iter, J)\n",
    "    \n",
    "    dZ = A - Y                 # dZ is (55000, 10)\n",
    "\n",
    "\n",
    "    dw = (1/m) * X.T @ dZ                #dw is (784, 10) \n",
    "    db = (1/m) * np.sum(dZ,axis=0, keepdims=True)\n",
    "\n",
    "    w -= alpha*dw\n",
    "    b -= alpha*db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise 6 - Compute the accuracy\n",
    "--\n",
    "Compute the accuracy of softmax classification on the train and test datasets.\n",
    "\n",
    "Use np.argmax(A, 1) and np.argmax(Y, 1) to find the predicted and real class for each example. They return an array of numbers each, e.g. [7 3 9 ..., 8 0 8] and [7 3 4 ..., 5 6 8]. Compare them using ==. You will get an array of booleans, e.g. [ True  True False ..., False False  True]. Sum up the latter using np.sum(). True values will be considered 1, False values will be considered 0, so the sum will tell us how many True values we got. Then divide by Y.shape[0] and multiply by 100 to get the accuracy in percentage. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7 3 4 ..., 5 6 8]\n",
      "Accuracy on the train set is  9.89818181818\n",
      "Accuracy on the test set is  9.8\n"
     ]
    }
   ],
   "source": [
    "def accuracy(A, Y):\n",
    "    return 100*np.sum(np.argmax(A, 1) == np.argmax(Y, 1)) / Y.shape[0]\n",
    "\n",
    "Z = X @ w + b\n",
    "A = softmax(Z)\n",
    "\n",
    "print(np.argmax(Y, 1))\n",
    "\n",
    "Z_test = X_test @ w + b\n",
    "A_test = softmax(Z_test)\n",
    "\n",
    "print(\"Accuracy on the train set is \", accuracy(A,Y))\n",
    "print(\"Accuracy on the test set is \", accuracy(A_test,Y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise 7 - Implement the Softmax classifier on TensorFlow\n",
    "--\n",
    "Implementing the Softmax classifier on TensorFlow is very similar to the TensorFlow example for binary logistic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input data.\n",
    "# Load the training and test data into constants\n",
    "tf_X = tf.constant(X)\n",
    "tf_Y = tf.constant(Y)\n",
    "tf_X_test = tf.constant(X_test)\n",
    "tf_Y_test = tf.constant(Y_test)\n",
    "\n",
    "# Variables.\n",
    "# These are the parameters that we are going to be training.\n",
    "tf_w = tf.Variable( tf.zeros((n_x, C)) )\n",
    "tf_b = tf.Variable(tf.zeros((1,C)))\n",
    "\n",
    "# Training computation.\n",
    "# We multiply the inputs with the weight matrix, and add biases. We compute\n",
    "# the softmax and cross-entropy (it's one operation in TensorFlow, because\n",
    "# it's very common, and it can be optimized). We take the average of this\n",
    "# cross-entropy across all training examples: that's our cost.\n",
    "tf_Z = tf.matmul(tf_X, tf_w) + tf_b\n",
    "tf_J = tf.reduce_mean( tf.nn.softmax_cross_entropy_with_logits(labels=tf_Y, logits=tf_Z) )\n",
    "\n",
    "# Optimizer.\n",
    "# We are going to find the minimum of this loss using gradient descent.\n",
    "# We pass alpha=0.1 as input parameter.\n",
    "optimizer = tf.train.GradientDescentOptimizer(0.1).minimize(tf_J)\n",
    "\n",
    "# Predictions for the train and test data.\n",
    "# These are not part of training, but merely here so that we can report\n",
    "# accuracy figures as we train.\n",
    "tf_A = tf.nn.softmax(tf_Z)\n",
    "tf_A_test = tf.nn.softmax(tf.matmul(tf_X_test, tf_w) + tf_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "0 2.30271\n",
      "1 2.19706\n",
      "2 2.10086\n",
      "3 2.01204\n",
      "4 1.92974\n",
      "5 1.85342\n",
      "6 1.78266\n",
      "7 1.71708\n",
      "8 1.6563\n",
      "9 1.59996\n"
     ]
    }
   ],
   "source": [
    "session = tf.InteractiveSession()\n",
    "\n",
    "# This is a one-time operation which ensures the parameters get initialized as\n",
    "# we described in the graph: random weights for the matrix, zeros for the biases. \n",
    "tf.global_variables_initializer().run()\n",
    "print(\"Initialized\")\n",
    "\n",
    "for iter in range(10):\n",
    "    # Run the computations. We tell .run() that we want to run the optimizer,\n",
    "    # and get the cost value and the training predictions returned as numpy arrays.\n",
    "    _, J, A = session.run([optimizer, tf_J, tf_A])\n",
    "    \n",
    "    print(iter, J)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on the train set is  77.5290909091\n",
      "Accuracy on the test set is  79.02\n"
     ]
    }
   ],
   "source": [
    "# Calling .eval() is basically like calling run(), but\n",
    "# just to get that one numpy array. \n",
    "# Note that it recomputes all its computation graph dependencies.\n",
    "A = tf_A.eval()\n",
    "A_test = tf_A_test.eval()\n",
    "\n",
    "print(\"Accuracy on the train set is \", accuracy(A,Y))\n",
    "print(\"Accuracy on the test set is \", accuracy(A_test,Y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise 8 - Implement a neural network with one hidden layer in Tensorflow.\n",
    "--\n",
    "Turn the previous exercise into a 1-hidden layer neural network with rectified linear units and 15 hidden nodes. The output layer should continue to be softmax.\n",
    "\n",
    "You need to include another set of weights for the hidden layer. \n",
    "The hidden layer should be fully connected to the input layer and to the output layer. \n",
    "Use a learning rate of 0.5 and perform 300 iterations. \n",
    "\n",
    "To get a comparable accuracy with the simple softmax classifier, you should run at least 1001 iterations. Try it out if you have time (it will take some time to run). \n",
    "Print out the cost every 50 iterations.\n",
    "Depending on your machine, the training process could take several minutes.\n",
    "The cost should decrease in every iteration. If not, you have some bug in the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_hidden_nodes = 15\n",
    "\n",
    "# Input data.\n",
    "# Load the training and test data into constants\n",
    "tf_X = tf.constant(X)\n",
    "tf_Y = tf.constant(Y)\n",
    "tf_X_test = tf.constant(X_test)\n",
    "tf_Y_test = tf.constant(Y_test)\n",
    "\n",
    "# Variables.\n",
    "# These are the parameters that we are going to be training. The weight\n",
    "# matrices will be initialized using random values following a (truncated)\n",
    "# normal distribution. The biases get initialized to zero.\n",
    "tf_w1 = tf.Variable( tf.truncated_normal((n_x, num_hidden_nodes)) )\n",
    "tf_b1 = tf.Variable(tf.zeros((1,num_hidden_nodes)))\n",
    "tf_w2 = tf.Variable(tf.truncated_normal((num_hidden_nodes, C)))\n",
    "tf_b2 = tf.Variable(tf.zeros((1,C)))\n",
    "\n",
    "# Forward computation.\n",
    "tf_Z1 = tf.matmul(tf_X, tf_w1) + tf_b1\n",
    "tf_A1 = tf.nn.relu(tf_Z1)\n",
    "tf_Z2 = tf.matmul(tf_A1, tf_w2) + tf_b2\n",
    "tf_A2 = tf.nn.softmax(tf_Z2)\n",
    "\n",
    "tf_J = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=tf_Z2, labels=tf_Y))\n",
    "\n",
    "# Optimizer.\n",
    "optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(tf_J)\n",
    "\n",
    "# Predictions for the test data.\n",
    "tf_Z1_test = tf.matmul(tf_X_test, tf_w1) + tf_b1\n",
    "tf_A1_test = tf.nn.relu(tf_Z1_test)\n",
    "tf_Z2_test = tf.matmul(tf_A1_test, tf_w2) + tf_b2\n",
    "tf_A2_test = tf.nn.softmax(tf_Z2_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "0 24.16\n",
      "50 1.61174\n",
      "100 1.29295\n",
      "150 1.08237\n",
      "200 0.934851\n",
      "250 0.843148\n"
     ]
    }
   ],
   "source": [
    "session = tf.InteractiveSession()\n",
    "\n",
    "# This is a one-time operation which ensures the parameters get initialized as\n",
    "# we described in the graph: random weights for the matrix, zeros for the biases. \n",
    "tf.global_variables_initializer().run()\n",
    "print(\"Initialized\")\n",
    "\n",
    "for iter in range(300):\n",
    "    # Run the computations. We tell .run() that we want to run the optimizer,\n",
    "    # and get the cost value and the training predictions returned as numpy arrays.\n",
    "    _, J, A = session.run([optimizer, tf_J, tf_A2])\n",
    "    \n",
    "    if iter%50 == 0: \n",
    "        print(iter, J)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on the train set is  75.1527272727\n",
      "Accuracy on the test set is  76.44\n"
     ]
    }
   ],
   "source": [
    "A = tf_A2.eval()\n",
    "A_test = tf_A2_test.eval()\n",
    "\n",
    "print(\"Accuracy on the train set is \", accuracy(A,Y))\n",
    "print(\"Accuracy on the test set is \", accuracy(A_test,Y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise 9 - Stochastic Gradient Descent\n",
    "--\n",
    "As you observed above, 300 iterations are not enough to reach a good accuracy when using more layers, i.e. more weights (parameters) to train. We need more iterations and so it takes more time to train the network.\n",
    "\n",
    "Here we want to do batch stochastic gradient descent and approximate the gradient using batches of training examples. This will make training the network much faster, and we can train for many more iterations in a shorter time. \n",
    "\n",
    "Set num_steps = 5001 (number of iterations) and batch_size = 100. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_hidden_nodes = 15\n",
    "\n",
    "# Input data.\n",
    "# Let's use placeholders for the training data. \n",
    "# This is so that we can suply batches of tranining examples each iteration.\n",
    "tf_X = tf.placeholder(tf.float32)\n",
    "tf_Y = tf.placeholder(tf.float32)\n",
    "\n",
    "# Variables.\n",
    "# These are the parameters that we are going to be training. The weight\n",
    "# matrices will be initialized using random values following a (truncated)\n",
    "# normal distribution. The biases get initialized to zero.\n",
    "tf_w1 = tf.Variable( tf.truncated_normal((n_x, num_hidden_nodes)) )\n",
    "tf_b1 = tf.Variable(tf.zeros((1,num_hidden_nodes)))\n",
    "tf_w2 = tf.Variable(tf.truncated_normal([num_hidden_nodes, C]))\n",
    "tf_b2 = tf.Variable(tf.zeros((1,C)))\n",
    "\n",
    "# Forward computation.\n",
    "tf_Z1 = tf.matmul(tf_X, tf_w1) + tf_b1\n",
    "tf_A1 = tf.nn.relu(tf_Z1)\n",
    "tf_Z2 = tf.matmul(tf_A1, tf_w2) + tf_b2\n",
    "tf_A2 = tf.nn.softmax(tf_Z2)\n",
    "\n",
    "tf_J = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=tf_Z2, labels=tf_Y))\n",
    "\n",
    "# Optimizer.\n",
    "optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(tf_J)\n",
    "\n",
    "# Predictions for the test data.\n",
    "tf_Z1_test = tf.matmul(tf_X_test, tf_w1) + tf_b1\n",
    "tf_A1_test = tf.nn.relu(tf_Z1_test)\n",
    "tf_Z2_test = tf.matmul(tf_A1_test, tf_w2) + tf_b2\n",
    "tf_A2_test = tf.nn.softmax(tf_Z2_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step  (0, 38.013912)\n",
      "Minibatch accuracy:  9.0\n",
      "Test accuracy:  13.34\n",
      "Minibatch loss at step  (500, 0.8238138)\n",
      "Minibatch accuracy:  70.0\n",
      "Test accuracy:  67.25\n",
      "Minibatch loss at step  (1000, 0.61109972)\n",
      "Minibatch accuracy:  77.0\n",
      "Test accuracy:  79.12\n",
      "Minibatch loss at step  (1500, 0.48295361)\n",
      "Minibatch accuracy:  83.0\n",
      "Test accuracy:  82.59\n",
      "Minibatch loss at step  (2000, 0.43151048)\n",
      "Minibatch accuracy:  87.0\n",
      "Test accuracy:  83.91\n",
      "Minibatch loss at step  (2500, 0.39448601)\n",
      "Minibatch accuracy:  90.0\n",
      "Test accuracy:  87.21\n",
      "Minibatch loss at step  (3000, 0.3518438)\n",
      "Minibatch accuracy:  87.0\n",
      "Test accuracy:  86.95\n",
      "Minibatch loss at step  (3500, 0.3546536)\n",
      "Minibatch accuracy:  89.0\n",
      "Test accuracy:  89.28\n",
      "Minibatch loss at step  (4000, 0.50670648)\n",
      "Minibatch accuracy:  88.0\n",
      "Test accuracy:  87.96\n",
      "Minibatch loss at step  (4500, 0.32419768)\n",
      "Minibatch accuracy:  89.0\n",
      "Test accuracy:  90.65\n",
      "Minibatch loss at step  (5000, 0.28202674)\n",
      "Minibatch accuracy:  94.0\n",
      "Test accuracy:  90.42\n"
     ]
    }
   ],
   "source": [
    "num_steps = 5001\n",
    "batch_size = 100\n",
    "\n",
    "tf.global_variables_initializer().run()\n",
    "print(\"Initialized\")\n",
    "\n",
    "for step in range(num_steps):\n",
    "    # Pick an offset within the training data.\n",
    "    offset = (step * batch_size) % (X.shape[0] - batch_size)\n",
    "    \n",
    "    # Generate a minibatch.\n",
    "    X_batch = X[offset:(offset + batch_size), :]\n",
    "    Y_batch = Y[offset:(offset + batch_size), :]\n",
    "    \n",
    "    _, J, A = session.run([optimizer, tf_J, tf_A2], feed_dict={tf_X : X_batch, tf_Y : Y_batch})\n",
    "    \n",
    "    if (step % 500 == 0):\n",
    "        print(\"Minibatch loss at step \", (step, J))\n",
    "        print(\"Minibatch accuracy: \", accuracy(A, Y_batch))\n",
    "        A_test = tf_A2_test.eval()\n",
    "        print(\"Test accuracy: \", accuracy(A_test,Y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, just for curiosity, make the number of hidden units above quite larger, e.g. 1000, and run the training again. Yes, we can do that using batch SGD and be able to train the quite large network in a reasonable time (several minutes).\n",
    "\n",
    "What's the accuracy you get? "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
